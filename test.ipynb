{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131072\n"
     ]
    }
   ],
   "source": [
    "model_path = \"hf-internal-testing/Mixtral-tiny\"\n",
    "token_path = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, MixtralModel, MixtralConfig, FlaxMixtralModel\n",
    "config = MixtralConfig.from_pretrained(model_path)\n",
    "config.output_router_logits = True\n",
    "config.output_attentions = True\n",
    "config.output_hidden_states = True\n",
    "print(config.max_position_embeddings)\n",
    "config.max_position_embeddings = 4096\n",
    "# print(config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MixtralModel.from_pretrained(model_path, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(token_path)\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "x = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixtralModel(\n",
      "  (embed_tokens): Embedding(32000, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x MixtralDecoderLayer(\n",
      "      (self_attn): MixtralSdpaAttention(\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (rotary_emb): MixtralRotaryEmbedding()\n",
      "      )\n",
      "      (block_sparse_moe): MixtralSparseMoeBlock(\n",
      "        (gate): Linear(in_features=1024, out_features=8, bias=False)\n",
      "        (experts): ModuleList(\n",
      "          (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
      "            (w1): Linear(in_features=1024, out_features=3584, bias=False)\n",
      "            (w2): Linear(in_features=3584, out_features=1024, bias=False)\n",
      "            (w3): Linear(in_features=1024, out_features=3584, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (input_layernorm): MixtralRMSNorm()\n",
      "      (post_attention_layernorm): MixtralRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): MixtralRMSNorm()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214193152"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4910,  1.2440,  2.3478,  ...,  1.4659, -1.9996,  2.3321],\n",
       "         [-1.4066, -1.2585, -0.1159,  ...,  0.4500,  0.3790,  0.8595],\n",
       "         [-2.1107,  0.0814, -0.1963,  ..., -0.8745, -0.6745, -0.0833],\n",
       "         ...,\n",
       "         [ 0.2947, -1.0839,  0.3771,  ..., -0.5531, -1.0748,  0.6421],\n",
       "         [-0.1680, -0.6061,  1.2885,  ..., -1.1196,  1.1017, -0.3771],\n",
       "         [ 0.7433,  0.3478,  0.2059,  ...,  1.2677,  0.0713, -0.5899]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/Mixtral-tiny were not used when initializing FlaxMixtralModel: {('lm_head', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxMixtralModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxMixtralModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some of the weights of FlaxMixtralModel were initialized in float16 precision from the model checkpoint at hf-internal-testing/Mixtral-tiny:\n",
      "[('embed_tokens', 'embedding'), ('layers', '0', 'block_sparse_moe', 'experts', '0', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '0', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '0', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '1', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '1', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '1', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '2', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '2', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '2', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '3', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '3', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '3', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '4', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '4', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '4', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '5', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '5', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '5', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '6', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '6', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '6', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '7', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '7', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '7', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'gate', 'kernel'), ('layers', '0', 'input_layernorm', 'weight'), ('layers', '0', 'post_attention_layernorm', 'weight'), ('layers', '0', 'self_attn', 'k_proj', 'kernel'), ('layers', '0', 'self_attn', 'o_proj', 'kernel'), ('layers', '0', 'self_attn', 'q_proj', 'kernel'), ('layers', '0', 'self_attn', 'v_proj', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '0', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '0', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '0', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '1', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '1', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '1', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '2', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '2', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '2', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '3', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '3', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '3', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '4', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '4', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '4', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '5', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '5', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '5', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '6', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '6', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '6', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '7', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '7', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '7', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'gate', 'kernel'), ('layers', '1', 'input_layernorm', 'weight'), ('layers', '1', 'post_attention_layernorm', 'weight'), ('layers', '1', 'self_attn', 'k_proj', 'kernel'), ('layers', '1', 'self_attn', 'o_proj', 'kernel'), ('layers', '1', 'self_attn', 'q_proj', 'kernel'), ('layers', '1', 'self_attn', 'v_proj', 'kernel'), ('norm', 'weight')]\n",
      "You should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\n"
     ]
    }
   ],
   "source": [
    "model = FlaxMixtralModel.from_pretrained(model_path, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(token_path)\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"jax\")\n",
    "\n",
    "# y = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    embed_tokens: {\n",
      "        embedding: ShapeDtypeStruct(shape=(32000, 1024), dtype=float32),\n",
      "    },\n",
      "    layers: {\n",
      "        0: {\n",
      "            block_sparse_moe: {\n",
      "                experts: {\n",
      "                    0: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    1: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    2: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    3: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    4: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    5: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    6: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    7: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                },\n",
      "                gate: {\n",
      "                    kernel: ShapeDtypeStruct(shape=(1024, 8), dtype=float32),\n",
      "                },\n",
      "            },\n",
      "            input_layernorm: {\n",
      "                weight: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
      "            },\n",
      "            post_attention_layernorm: {\n",
      "                weight: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
      "            },\n",
      "            self_attn: {\n",
      "                k_proj: {\n",
      "                    kernel: ShapeDtypeStruct(shape=(1024, 256), dtype=float32),\n",
      "                },\n",
      "                o_proj: {\n",
      "                    kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
      "                },\n",
      "                q_proj: {\n",
      "                    kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
      "                },\n",
      "                v_proj: {\n",
      "                    kernel: ShapeDtypeStruct(shape=(1024, 256), dtype=float32),\n",
      "                },\n",
      "            },\n",
      "        },\n",
      "        1: {\n",
      "            block_sparse_moe: {\n",
      "                experts: {\n",
      "                    0: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    1: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    2: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    3: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    4: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    5: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    6: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                    7: {\n",
      "                        w1: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                        w2: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(3584, 1024), dtype=float32),\n",
      "                        },\n",
      "                        w3: {\n",
      "                            kernel: ShapeDtypeStruct(shape=(1024, 3584), dtype=float32),\n",
      "                        },\n",
      "                    },\n",
      "                },\n",
      "                gate: {\n",
      "                    kernel: ShapeDtypeStruct(shape=(1024, 8), dtype=float32),\n",
      "                },\n",
      "            },\n",
      "            input_layernorm: {\n",
      "                weight: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
      "            },\n",
      "            post_attention_layernorm: {\n",
      "                weight: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
      "            },\n",
      "            self_attn: {\n",
      "                k_proj: {\n",
      "                    kernel: ShapeDtypeStruct(shape=(1024, 256), dtype=float32),\n",
      "                },\n",
      "                o_proj: {\n",
      "                    kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
      "                },\n",
      "                q_proj: {\n",
      "                    kernel: ShapeDtypeStruct(shape=(1024, 1024), dtype=float32),\n",
      "                },\n",
      "                v_proj: {\n",
      "                    kernel: ShapeDtypeStruct(shape=(1024, 256), dtype=float32),\n",
      "                },\n",
      "            },\n",
      "        },\n",
      "    },\n",
      "    norm: {\n",
      "        weight: ShapeDtypeStruct(shape=(1024,), dtype=float32),\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(model._params_shape_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[-1.4910138 ,  1.2440001 ,  2.347837  , ...,  1.4658867 ,\n",
       "         -1.9995558 ,  2.3321059 ],\n",
       "        [-1.4065796 , -1.2585348 , -0.11586622, ...,  0.4500268 ,\n",
       "          0.37901428,  0.8594837 ],\n",
       "        [-2.1106687 ,  0.08143321, -0.19634205, ..., -0.87450725,\n",
       "         -0.67447233, -0.08327945],\n",
       "        ...,\n",
       "        [ 0.2947395 , -1.0838611 ,  0.377108  , ..., -0.55314595,\n",
       "         -1.074801  ,  0.6420861 ],\n",
       "        [-0.16801266, -0.60607636,  1.2884645 , ..., -1.1196154 ,\n",
       "          1.1017209 , -0.37705478],\n",
       "        [ 0.7433374 ,  0.3478445 ,  0.20590468, ...,  1.2677363 ,\n",
       "          0.07131021, -0.5898897 ]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = []\n",
    "for i in range(len(x.attentions)):\n",
    "        for j in range(len(x.attentions[i])):\n",
    "                for k in range(len(x.attentions[i][j])):\n",
    "                        for l in range(len(x.attentions[i][j][k])):\n",
    "                                to_keep.append(np.amax(np.abs(np.array(x.attentions[i][j][k][l].detach().cpu()) - np.array(y.attentions[i][j][k][l]))))\n",
    "\n",
    "# for i in range(len(x.last_hidden_state)):\n",
    "#         for j in range(len(x.last_hidden_state[i])):\n",
    "#                                 to_keep.append(np.amax(np.abs(np.array(x.attentions[i][j].detach().cpu()) - np.array(y.attentions[i][j]))))\n",
    "\n",
    "to_keep.append(np.amax(np.abs(np.array(x.attentions[0].detach().cpu()) - np.array(y.attentions[0]))))\n",
    "\n",
    "\n",
    "for i in range(len(x.hidden_states)):\n",
    "        for j in range(len(x.hidden_states[i])):\n",
    "                for k in range(len(x.hidden_states[i][j])):\n",
    "                                to_keep.append(np.amax(np.abs(np.array(x.hidden_states[i][j][k].detach().cpu()) - np.array(y.hidden_states[i][j][k]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = []\n",
    "for i in range(len(x.attentions)):\n",
    "        for j in range(len(x.attentions[i])):\n",
    "                for k in range(len(x.attentions[i][j])):\n",
    "                        for l in range(len(x.attentions[i][j][k])):\n",
    "                                to_keep.append(np.amax(np.abs(np.array(x.attentions[i][j][k][l].detach().cpu()) - np.around(np.array(y.attentions[i][j][k][l]), 4))))\n",
    "\n",
    "# for i in range(len(x.last_hidden_state)):\n",
    "#         for j in range(len(x.last_hidden_state[i])):\n",
    "#                                 to_keep.append(np.amax(np.abs(np.array(x.attentions[i][j].detach().cpu()) - np.array(y.attentions[i][j]))))\n",
    "\n",
    "to_keep.append(np.amax(np.abs(np.array(x.attentions[0].detach().cpu()) - np.around(np.array(y.attentions[0]), 4))))\n",
    "\n",
    "\n",
    "for i in range(len(x.hidden_states)):\n",
    "        for j in range(len(x.hidden_states[i])):\n",
    "                for k in range(len(x.hidden_states[i][j])):\n",
    "                                to_keep.append(np.amax(np.abs(np.array(x.hidden_states[i][j][k].detach().cpu()) - np.around(np.array(y.hidden_states[i][j][k]), 4))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
