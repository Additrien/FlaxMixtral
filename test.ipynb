{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrien/Documents/flax/FlaxMixtral/.env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "MixtralModel is using MixtralSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse block:  tensor([[ 0.0565,  0.3033,  0.0746,  0.5048, -0.1273,  0.5146,  0.3725,  0.7331],\n",
      "        [-0.6925, -0.6075,  1.0916, -0.0951, -0.6929, -0.5925,  0.9077, -0.0924],\n",
      "        [ 0.5982, -0.8751, -0.2029, -0.1773,  0.1937,  0.4039,  0.8775, -0.3451],\n",
      "        [-0.0740, -0.7922,  0.0529,  0.9633, -0.0918,  0.3685,  0.0860, -0.0421],\n",
      "        [-1.0877,  0.0867,  0.0434,  0.0448,  0.6515,  1.2938,  0.2649, -1.2108],\n",
      "        [ 0.1927,  0.2432, -0.2782, -0.3695, -0.6098, -0.1551, -0.3344,  0.0463],\n",
      "        [ 0.3873,  0.1157, -0.2483, -0.6784, -0.7712, -0.8781,  0.0244,  0.2367],\n",
      "        [ 0.0341, -0.4348, -0.0610, -0.5961, -0.3237, -0.4529, -0.4729, -0.6828],\n",
      "        [-1.0326,  0.0998, -0.0245, -0.0839,  0.7239,  1.3174,  0.2752, -1.2005],\n",
      "        [-0.1753,  0.9458, -0.1117,  0.0430, -0.7539,  0.0218,  0.1420, -1.5287],\n",
      "        [ 0.2883, -0.5809, -0.3837,  0.2736,  0.2022,  0.4310, -0.0177,  0.1172],\n",
      "        [-0.6292,  0.3650,  0.7984, -0.7722,  0.9737, -0.1049, -0.4818, -0.3720],\n",
      "        [ 0.3602,  0.1337, -0.2507, -0.7024, -0.8170, -0.8017,  0.0263,  0.1869]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "decoder:  tensor([[ 0.0565,  0.3033,  0.0746,  0.5048, -0.1273,  0.5146,  0.3725,  0.7331],\n",
      "        [-0.6925, -0.6075,  1.0916, -0.0951, -0.6929, -0.5925,  0.9077, -0.0924],\n",
      "        [ 0.5982, -0.8751, -0.2029, -0.1773,  0.1937,  0.4039,  0.8775, -0.3451],\n",
      "        [-0.0740, -0.7922,  0.0529,  0.9633, -0.0918,  0.3685,  0.0860, -0.0421],\n",
      "        [-1.0877,  0.0867,  0.0434,  0.0448,  0.6515,  1.2938,  0.2649, -1.2108],\n",
      "        [ 0.1927,  0.2432, -0.2782, -0.3695, -0.6098, -0.1551, -0.3344,  0.0463],\n",
      "        [ 0.3873,  0.1157, -0.2483, -0.6784, -0.7712, -0.8781,  0.0244,  0.2367],\n",
      "        [ 0.0341, -0.4348, -0.0610, -0.5961, -0.3237, -0.4529, -0.4729, -0.6828],\n",
      "        [-1.0326,  0.0998, -0.0245, -0.0839,  0.7239,  1.3174,  0.2752, -1.2005],\n",
      "        [-0.1753,  0.9458, -0.1117,  0.0430, -0.7539,  0.0218,  0.1420, -1.5287],\n",
      "        [ 0.2883, -0.5809, -0.3837,  0.2736,  0.2022,  0.4310, -0.0177,  0.1172],\n",
      "        [-0.6292,  0.3650,  0.7984, -0.7722,  0.9737, -0.1049, -0.4818, -0.3720],\n",
      "        [ 0.3602,  0.1337, -0.2507, -0.7024, -0.8170, -0.8017,  0.0263,  0.1869]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "model:  (tensor([[ 0.0565,  0.3033,  0.0746,  0.5048, -0.1273,  0.5146,  0.3725,  0.7331],\n",
      "        [-0.6925, -0.6075,  1.0916, -0.0951, -0.6929, -0.5925,  0.9077, -0.0924],\n",
      "        [ 0.5982, -0.8751, -0.2029, -0.1773,  0.1937,  0.4039,  0.8775, -0.3451],\n",
      "        [-0.0740, -0.7922,  0.0529,  0.9633, -0.0918,  0.3685,  0.0860, -0.0421],\n",
      "        [-1.0877,  0.0867,  0.0434,  0.0448,  0.6515,  1.2938,  0.2649, -1.2108],\n",
      "        [ 0.1927,  0.2432, -0.2782, -0.3695, -0.6098, -0.1551, -0.3344,  0.0463],\n",
      "        [ 0.3873,  0.1157, -0.2483, -0.6784, -0.7712, -0.8781,  0.0244,  0.2367],\n",
      "        [ 0.0341, -0.4348, -0.0610, -0.5961, -0.3237, -0.4529, -0.4729, -0.6828],\n",
      "        [-1.0326,  0.0998, -0.0245, -0.0839,  0.7239,  1.3174,  0.2752, -1.2005],\n",
      "        [-0.1753,  0.9458, -0.1117,  0.0430, -0.7539,  0.0218,  0.1420, -1.5287],\n",
      "        [ 0.2883, -0.5809, -0.3837,  0.2736,  0.2022,  0.4310, -0.0177,  0.1172],\n",
      "        [-0.6292,  0.3650,  0.7984, -0.7722,  0.9737, -0.1049, -0.4818, -0.3720],\n",
      "        [ 0.3602,  0.1337, -0.2507, -0.7024, -0.8170, -0.8017,  0.0263,  0.1869]],\n",
      "       grad_fn=<MmBackward0>),)\n",
      "sparse block:  tensor([[ 0.5981,  0.7106, -0.5592,  0.8182, -0.1869,  0.7544, -0.6112, -0.5552],\n",
      "        [-0.0141,  0.6522,  0.3090, -0.4479, -1.0477,  0.0088, -0.3927,  0.4015],\n",
      "        [-0.6747,  0.4317, -0.1052,  0.0947,  0.4313, -0.6400,  0.0240, -0.0599],\n",
      "        [-0.7821, -0.1986,  0.5397, -0.5771, -0.4929,  0.8380, -0.3100,  0.4826],\n",
      "        [-0.8568,  0.3792, -0.6077, -1.4482, -0.3714, -0.0639,  0.5102, -0.7657],\n",
      "        [-0.0884, -0.0838,  0.3500, -0.5429, -0.1628,  0.3780,  0.5746, -0.3300],\n",
      "        [-0.4687, -0.3363,  0.2030, -0.2781,  0.1441,  0.9680,  1.4434,  0.1515],\n",
      "        [-1.1709,  0.4601,  1.3775,  0.0246,  0.9720,  1.1061,  0.5382,  0.5144],\n",
      "        [-0.9586,  0.3181, -0.5981, -1.4597, -0.3218, -0.1091,  0.5706, -0.7879],\n",
      "        [ 0.4814,  0.0928, -0.6983,  0.2422, -0.2921,  0.2984, -0.2927,  1.1060],\n",
      "        [-0.4165,  0.3224, -0.0240, -0.1188, -0.1994,  0.7911,  0.0137, -0.1500],\n",
      "        [ 0.5503,  0.4668,  0.6461, -0.4320, -0.1612,  0.1744, -0.0535, -0.5017],\n",
      "        [-0.4816, -0.3700,  0.1838, -0.2000,  0.1937,  0.9954,  1.4288,  0.1595]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "decoder:  tensor([[ 0.5981,  0.7106, -0.5592,  0.8182, -0.1869,  0.7544, -0.6112, -0.5552],\n",
      "        [-0.0141,  0.6522,  0.3090, -0.4479, -1.0477,  0.0088, -0.3927,  0.4015],\n",
      "        [-0.6747,  0.4317, -0.1052,  0.0947,  0.4313, -0.6400,  0.0240, -0.0599],\n",
      "        [-0.7821, -0.1986,  0.5397, -0.5771, -0.4929,  0.8380, -0.3100,  0.4826],\n",
      "        [-0.8568,  0.3792, -0.6077, -1.4482, -0.3714, -0.0639,  0.5102, -0.7657],\n",
      "        [-0.0884, -0.0838,  0.3500, -0.5429, -0.1628,  0.3780,  0.5746, -0.3300],\n",
      "        [-0.4687, -0.3363,  0.2030, -0.2781,  0.1441,  0.9680,  1.4434,  0.1515],\n",
      "        [-1.1709,  0.4601,  1.3775,  0.0246,  0.9720,  1.1061,  0.5382,  0.5144],\n",
      "        [-0.9586,  0.3181, -0.5981, -1.4597, -0.3218, -0.1091,  0.5706, -0.7879],\n",
      "        [ 0.4814,  0.0928, -0.6983,  0.2422, -0.2921,  0.2984, -0.2927,  1.1060],\n",
      "        [-0.4165,  0.3224, -0.0240, -0.1188, -0.1994,  0.7911,  0.0137, -0.1500],\n",
      "        [ 0.5503,  0.4668,  0.6461, -0.4320, -0.1612,  0.1744, -0.0535, -0.5017],\n",
      "        [-0.4816, -0.3700,  0.1838, -0.2000,  0.1937,  0.9954,  1.4288,  0.1595]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "model:  (tensor([[ 0.5981,  0.7106, -0.5592,  0.8182, -0.1869,  0.7544, -0.6112, -0.5552],\n",
      "        [-0.0141,  0.6522,  0.3090, -0.4479, -1.0477,  0.0088, -0.3927,  0.4015],\n",
      "        [-0.6747,  0.4317, -0.1052,  0.0947,  0.4313, -0.6400,  0.0240, -0.0599],\n",
      "        [-0.7821, -0.1986,  0.5397, -0.5771, -0.4929,  0.8380, -0.3100,  0.4826],\n",
      "        [-0.8568,  0.3792, -0.6077, -1.4482, -0.3714, -0.0639,  0.5102, -0.7657],\n",
      "        [-0.0884, -0.0838,  0.3500, -0.5429, -0.1628,  0.3780,  0.5746, -0.3300],\n",
      "        [-0.4687, -0.3363,  0.2030, -0.2781,  0.1441,  0.9680,  1.4434,  0.1515],\n",
      "        [-1.1709,  0.4601,  1.3775,  0.0246,  0.9720,  1.1061,  0.5382,  0.5144],\n",
      "        [-0.9586,  0.3181, -0.5981, -1.4597, -0.3218, -0.1091,  0.5706, -0.7879],\n",
      "        [ 0.4814,  0.0928, -0.6983,  0.2422, -0.2921,  0.2984, -0.2927,  1.1060],\n",
      "        [-0.4165,  0.3224, -0.0240, -0.1188, -0.1994,  0.7911,  0.0137, -0.1500],\n",
      "        [ 0.5503,  0.4668,  0.6461, -0.4320, -0.1612,  0.1744, -0.0535, -0.5017],\n",
      "        [-0.4816, -0.3700,  0.1838, -0.2000,  0.1937,  0.9954,  1.4288,  0.1595]],\n",
      "       grad_fn=<MmBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, MixtralModel, MixtralConfig\n",
    "config = MixtralConfig.from_pretrained(\"hf-internal-testing/Mixtral-tiny\")\n",
    "config.output_router_logits = True\n",
    "config.output_attentions = True\n",
    "model = MixtralModel.from_pretrained(\"hf-internal-testing/Mixtral-tiny\", config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "x = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_outputs:  (Array([[[-0.30760983,  0.00448915, -1.7763262 , ...,  1.4183437 ,\n",
      "          0.651004  , -0.4189158 ]]], dtype=float32),)\n",
      "layer_outputs:  (Array([[[-1.094088 ,  1.0546091, -3.0550554, ...,  2.038435 ,\n",
      "         -0.6232225, -1.5848573]]], dtype=float32),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hf-internal-testing/Mixtral-tiny were not used when initializing FlaxMixtralModel: {('lm_head', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxMixtralModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxMixtralModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some of the weights of FlaxMixtralModel were initialized in float16 precision from the model checkpoint at hf-internal-testing/Mixtral-tiny:\n",
      "[('embed_tokens', 'embedding'), ('layers', '0', 'block_sparse_moe', 'experts', '0', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '0', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '0', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '1', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '1', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '1', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '2', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '2', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '2', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '3', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '3', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '3', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '4', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '4', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '4', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '5', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '5', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '5', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '6', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '6', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '6', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '7', 'w1', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '7', 'w2', 'kernel'), ('layers', '0', 'block_sparse_moe', 'experts', '7', 'w3', 'kernel'), ('layers', '0', 'block_sparse_moe', 'gate', 'kernel'), ('layers', '0', 'input_layernorm', 'weight'), ('layers', '0', 'post_attention_layernorm', 'weight'), ('layers', '0', 'self_attn', 'k_proj', 'kernel'), ('layers', '0', 'self_attn', 'o_proj', 'kernel'), ('layers', '0', 'self_attn', 'q_proj', 'kernel'), ('layers', '0', 'self_attn', 'v_proj', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '0', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '0', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '0', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '1', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '1', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '1', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '2', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '2', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '2', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '3', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '3', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '3', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '4', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '4', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '4', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '5', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '5', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '5', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '6', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '6', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '6', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '7', 'w1', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '7', 'w2', 'kernel'), ('layers', '1', 'block_sparse_moe', 'experts', '7', 'w3', 'kernel'), ('layers', '1', 'block_sparse_moe', 'gate', 'kernel'), ('layers', '1', 'input_layernorm', 'weight'), ('layers', '1', 'post_attention_layernorm', 'weight'), ('layers', '1', 'self_attn', 'k_proj', 'kernel'), ('layers', '1', 'self_attn', 'o_proj', 'kernel'), ('layers', '1', 'self_attn', 'q_proj', 'kernel'), ('layers', '1', 'self_attn', 'v_proj', 'kernel'), ('norm', 'weight')]\n",
      "You should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_outputs:  (Array([[[-0.9512765 ,  1.3742137 ,  2.0909903 , ...,  1.8151293 ,\n",
      "         -2.2085001 ,  1.4656765 ],\n",
      "        [-1.0045583 , -1.0591412 , -0.16560225, ...,  0.54093945,\n",
      "          0.43313074,  0.41004956],\n",
      "        [-1.5399573 ,  0.14566039, -0.18943106, ..., -0.7365563 ,\n",
      "         -0.6916512 , -0.4430975 ],\n",
      "        ...,\n",
      "        [ 0.44989714, -1.0703536 ,  0.5446242 , ..., -0.7091343 ,\n",
      "         -1.0767226 ,  0.69930696],\n",
      "        [-0.14029716, -0.5699066 ,  1.3213382 , ..., -1.2383035 ,\n",
      "          0.9742793 , -0.3317961 ],\n",
      "        [ 0.6119936 ,  0.39994478,  0.2614372 , ...,  1.2715026 ,\n",
      "          0.08781135, -0.5480071 ]]], dtype=float32),)\n",
      "layer_outputs:  (Array([[[-1.6238902 ,  1.354863  ,  2.5570717 , ...,  1.5965238 ,\n",
      "         -2.1777525 ,  2.5399387 ],\n",
      "        [-1.4560984 , -1.3028417 , -0.1199453 , ...,  0.46587005,\n",
      "          0.39235753,  0.88974196],\n",
      "        [-2.1907694 ,  0.08452363, -0.20379332, ..., -0.9076951 ,\n",
      "         -0.70006883, -0.08643994],\n",
      "        ...,\n",
      "        [ 0.29714692, -1.092714  ,  0.38018817, ..., -0.557664  ,\n",
      "         -1.0835798 ,  0.6473306 ],\n",
      "        [-0.17139105, -0.61826336,  1.314373  , ..., -1.1421287 ,\n",
      "          1.1238743 , -0.38463658],\n",
      "        [ 0.7745714 ,  0.36246046,  0.21455652, ...,  1.3210049 ,\n",
      "          0.07430656, -0.61467606]]], dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, FlaxMixtralModel, MixtralConfig\n",
    "\n",
    "config = MixtralConfig.from_pretrained(\"hf-internal-testing/Mixtral-tiny\")\n",
    "config.output_router_logits = True\n",
    "# config.output_attentions = True\n",
    "# config.past_key_value = True\n",
    "model = FlaxMixtralModel.from_pretrained(\"hf-internal-testing/Mixtral-tiny\", config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"jax\")\n",
    "\n",
    "y = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.router_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, FlaxMixtralForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "model = FlaxMixtralForCausalLM.from_pretrained(\"hf-internal-testing/Mixtral-tiny\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# retrieve logts for next token\n",
    "# next_token_logits = outputs.logits[:, -1]\n",
    "# next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in outputs:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
